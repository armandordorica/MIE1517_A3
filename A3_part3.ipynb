{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jda8pXEQ4HKa"
   },
   "source": [
    "## Part 3 Exploration [6 Pt]\n",
    "\n",
    "At this point we have trained a few different generative models for our image colourization task with varying results. What makes this work exciting is that there many other approaches we could take. In this part of the assignment you are asked to consider some modifications you could make to improve the results and provide some comments on why those approaches could be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvHWPRKv0rNK"
   },
   "source": [
    "### Part (i) [2pt DISCUSSION]\n",
    "\n",
    "We've seen several times in this course how pretrained models could be used to improve performance on classification tasks. Do you think they could be beneficial in improving our results on the image colourization tasks? If so, would it be helpful to implement them in the discriminative and/or generative networks? Why or why not?\n",
    "\n",
    "Limit your answers to no more than 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wN0ln5E7f0v"
   },
   "source": [
    "> Pretrained models can greatly enhance image colorization by providing a head start in learning complex features. They're useful in both generative networks, for better feature extraction and transfer learning, and discriminative networks, for improved content understanding and adversarial training. Fine-tuning these models to the specific colorization task can lead to better and faster results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Vys9KXtMAHI"
   },
   "source": [
    "### Part (ii) [2pt DISCUSSION]\n",
    "The CIFAR10 images are 32x32 pixels in resolution. Do you believe that using similar images with a resolution of 250x250 would lead to better results. Why or why not?\n",
    "\n",
    "Limit your answers to no more than 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si2QAzSx7gxr"
   },
   "source": [
    "> More pixels allow models to capture finer textures and color nuances, resulting in richer colorization. However, this comes with increased computational demands and a higher risk of overfitting, especially if the dataset is limited. Careful training and sufficient data are essential to harness the benefits of higher resolution without compromising model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k71ooYz_WB_T"
   },
   "source": [
    "### Part (iii) [2pt DISCUSSION]\n",
    "\n",
    "A colour space is a choice of mapping of colours into three-dimensional coordinates. Some colours could be close together in one colour space, but further apart in others. The RGB colour space is probably the most familiar to you, the model used in in our regression colourization example computes squared error in RGB colour space. But, most state of the art colourization models\n",
    "do not use RGB colour space. How could using the RGB colour space be problematic? Your answer should relate how human perception of colour is different than the squared distance. You may use the Wikipedia article on colour space to help you answer the question.\n",
    "\n",
    "Limit your answers to no more than 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAmJyDlMWF0-"
   },
   "source": [
    "\n",
    "> RGB color space is not perceptually uniform, meaning equal changes in RGB values don't correspond to equal changes in perceived color. This misalignment can cause colorization models to emphasize differences that humans barely notice while overlooking more perceptible discrepancies. Color spaces like Lab better reflect human color perception by separating luminance from chrominance, allowing for color differences to be measured more consistently with how we see them. Thus, colorization in Lab space can yield more visually accurate results than RGB, which mixes color and brightness together, potentially skewing the model's color choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4-9G8bB1WK4"
   },
   "source": [
    "# PART C (Optional) - Bonus Challenge!\n",
    "\n",
    "This is an optional exercise for those that finish the assignment early and would like to perform a deeper exploration of the concepts introduced in the assignment.\n",
    "\n",
    "In Part A we constructed an autoencoder, specifically a UNet architecture with skip connections to learn to colourize images. In Part B completed the same task using conditional GANs. For this bonus challenge we will explore several other techniques that could be used to improve the model's performance on image colourization. A great tutorial on some of these different approaches can be found in a <a href=\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\">blog post by Moein Shariatnia</a>. It is highly recommended that you read the article before completing the following tasks:\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Modify your conditional GAN architecture to incorporate a pretrained ResNet model to speed up and improve the generative abilities for image colourization.\n",
    "2. Modify your conditional GAN architecture to incorporate the patch discriminator trained on local regions in the images.\n",
    "3. Modify your conditional GAN architecture to use higher resolution images of upto 256 x 256 resolution and compare the performance. Summarize how performance changes with increasing resolution of images. Hint: for this task you will need to use another dataset, perhaps COCO, or ImageNet.\n",
    "4. Modify your conditional GAN architecture to incorporate lab colour space image data representation instead of RGB data. Compare the performance of your model using RGB vs lab colour space data.\n",
    "\n",
    "Bonus marks will be provided based on the number of tasks completed and how well they are completed. Summarize below your results and anything intersting you learned from the steps that you completed. Bonus marks cannot be accumulated beyond a maximum assignment grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgILVilnggKT"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvZVSNFw1Zj4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYwI4RmFS2RB"
   },
   "source": [
    "### Saving to HTML\n",
    "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
    "\n",
    "(1) download your ipynb file by clicking on File->Download.ipynb\n",
    "\n",
    "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
    "\n",
    "(3) run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TrsqdNgS5ex"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "jupyter nbconvert --to html /content/A3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuXhlFlPTY7F"
   },
   "source": [
    "(4) the html file will be available for download in the temporary Google Colab storage\n",
    "\n",
    "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKmW9v0413iX"
   },
   "source": [
    "# Assignment Grading Rubric\n",
    "The grading of the assignment will be based on the following categories:\n",
    "\n",
    "(1) **10 Pt - EXPLORATORY QUESTIONS** These are basic questions that in most cases can be answered without requiring a fully working and trained neural network model. For example, data loading, processing and visualization, summary statistics, data exploration, model and training setup, etc.\n",
    "\n",
    "(2) **10 Pt - MODEL** Student has successfully implemented all the required neural network models and has demonstrated successful training of the model without any errors.\n",
    "\n",
    "(3) **10 Pt - RESULT** Students are evaluated based on the results achieved in comparison to the expected results of the assignment.\n",
    "\n",
    "(4) **10 Pt - DISCUSSION QUESTIONS** Student demonstrated understanding beyond the basic exploratory questions, can answer some of the more challenging questions, and provide arguments for their model selection decisions.\n",
    "\n",
    "(5) **10 Pt - COMMUNICATION** Student has provided a quality submission that is easy to read without too many unnecessary output statements that distract the reading of the document. The code has been well commented and all the answers are communicated clearly and concisely.\n",
    "\n",
    "(6) **10 Pt - BONUS** Student has completed the assignment and has taken on the challenging bonus tasks listed in PART C. The student has demonstrated a good understanding of all aspects of the assignment and has exceeded expectations for the assignment.\n",
    "\n",
    "\n",
    "\n",
    "**TOTAL GRADE = _____ of 50 Pts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
